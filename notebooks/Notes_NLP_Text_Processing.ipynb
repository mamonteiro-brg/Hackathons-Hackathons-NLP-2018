{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-28T14:40:56.281088Z",
     "start_time": "2018-07-28T14:40:55.836816Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import string\n",
    "from numpy import inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-28T14:40:56.984333Z",
     "start_time": "2018-07-28T14:40:56.283963Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-28T14:41:01.050958Z",
     "start_time": "2018-07-28T14:40:56.987351Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movi</th>\n",
       "      <th>film</th>\n",
       "      <th>one</th>\n",
       "      <th>like</th>\n",
       "      <th>time</th>\n",
       "      <th>get</th>\n",
       "      <th>watch</th>\n",
       "      <th>make</th>\n",
       "      <th>see</th>\n",
       "      <th>even</th>\n",
       "      <th>...</th>\n",
       "      <th>garlic</th>\n",
       "      <th>trilog</th>\n",
       "      <th>godspe</th>\n",
       "      <th>clad</th>\n",
       "      <th>keith</th>\n",
       "      <th>\".\"</th>\n",
       "      <th>fonda</th>\n",
       "      <th>bunni</th>\n",
       "      <th>..........</th>\n",
       "      <th>dupe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 5595 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   movi  film  one  like  time  get  watch  make  see  even  ...   garlic  \\\n",
       "0     3     5    0     0     2    1      1     3    0     0  ...        0   \n",
       "1     1     0    0     0     0    0      0     0    0     0  ...        0   \n",
       "2     6     0    2     1     0    0      0     0    0     1  ...        0   \n",
       "3     4     0    1     1     1    0      0     0    2     1  ...        0   \n",
       "4     1     0    1     0     1    1      1     0    4     0  ...        0   \n",
       "\n",
       "   trilog  godspe  clad  keith  \".\"  fonda  bunni  ..........  dupe  \n",
       "0       0       0     0      0    0      0      0           0     0  \n",
       "1       0       0     0      0    0      0      0           0     0  \n",
       "2       0       0     0      0    0      0      0           0     0  \n",
       "3       0       0     0      0    0      0      0           0     0  \n",
       "4       0       0     0      0    0      0      0           0     0  \n",
       "\n",
       "[5 rows x 5595 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Manual bag of words\n",
    "\n",
    "df = pd.read_csv('data_for_notes/imdb_sentiment.csv')\n",
    "docs = df.text[:200]\n",
    "\n",
    "\n",
    "def clean(doc):\n",
    "    # remove html tags\n",
    "    doc = re.sub(\"<[^>]*>\", \"\", doc)\n",
    "    # lowercase\n",
    "    doc = doc.lower()\n",
    "    # tokenize\n",
    "    words = tokenizer.tokenize(doc)\n",
    "    # remove punctuation\n",
    "    words = list(filter(lambda x: x not in string.punctuation, words))\n",
    "    # stem\n",
    "    stems = list(map(stemmer.stem, words))\n",
    "    new_doc = \" \".join(stems)\n",
    "    return new_doc\n",
    "\n",
    "\n",
    "def build_vocabulary(docs):\n",
    "    vocabulary = Counter()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    for doc in docs:\n",
    "        words = [word for word in doc.split() if word not in stop_words]\n",
    "        vocabulary.update(words)\n",
    "    \n",
    "    return OrderedDict(vocabulary.most_common())\n",
    "\n",
    "def vectorize(docs):\n",
    "    vocabulary = build_vocabulary(docs)\n",
    "    vectors = []\n",
    "    for doc in docs:\n",
    "        words = doc.split()\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        vector = np.array([doc.count(word) for word in vocabulary if word not in stop_words])\n",
    "        vectors.append(vector)\n",
    "        \n",
    "    return pd.DataFrame(vectors, columns=vocabulary)    \n",
    "\n",
    "\n",
    "\n",
    "tokenizer = WordPunctTokenizer()\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "clean_docs = docs.apply(clean)\n",
    "#print(clean_docs)\n",
    "bag_of_words = vectorize(clean_docs)\n",
    "bag_of_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-28T14:41:02.532155Z",
     "start_time": "2018-07-28T14:41:01.055020Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movi</th>\n",
       "      <th>film</th>\n",
       "      <th>one</th>\n",
       "      <th>like</th>\n",
       "      <th>time</th>\n",
       "      <th>get</th>\n",
       "      <th>watch</th>\n",
       "      <th>make</th>\n",
       "      <th>see</th>\n",
       "      <th>even</th>\n",
       "      <th>...</th>\n",
       "      <th>garlic</th>\n",
       "      <th>trilog</th>\n",
       "      <th>godspe</th>\n",
       "      <th>clad</th>\n",
       "      <th>keith</th>\n",
       "      <th>\".\"</th>\n",
       "      <th>fonda</th>\n",
       "      <th>bunni</th>\n",
       "      <th>..........</th>\n",
       "      <th>dupe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.007899</td>\n",
       "      <td>0.014278</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006853</td>\n",
       "      <td>0.003340</td>\n",
       "      <td>0.003761</td>\n",
       "      <td>0.010782</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.019593</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.026228</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008219</td>\n",
       "      <td>0.005169</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005876</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.018237</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004282</td>\n",
       "      <td>0.005379</td>\n",
       "      <td>0.005950</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010090</td>\n",
       "      <td>0.006115</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.002383</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002230</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.003019</td>\n",
       "      <td>0.003399</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010505</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 5595 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       movi      film       one      like      time       get     watch  \\\n",
       "0  0.007899  0.014278  0.000000  0.000000  0.006853  0.003340  0.003761   \n",
       "1  0.019593  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.026228  0.000000  0.008219  0.005169  0.000000  0.000000  0.000000   \n",
       "3  0.018237  0.000000  0.004282  0.005379  0.005950  0.000000  0.000000   \n",
       "4  0.002383  0.000000  0.002230  0.000000  0.003100  0.003019  0.003399   \n",
       "\n",
       "       make       see      even  ...   garlic  trilog  godspe  clad  keith  \\\n",
       "0  0.010782  0.000000  0.000000  ...      0.0     0.0     0.0   0.0    0.0   \n",
       "1  0.000000  0.000000  0.000000  ...      0.0     0.0     0.0   0.0    0.0   \n",
       "2  0.000000  0.000000  0.005876  ...      0.0     0.0     0.0   0.0    0.0   \n",
       "3  0.000000  0.010090  0.006115  ...      0.0     0.0     0.0   0.0    0.0   \n",
       "4  0.000000  0.010505  0.000000  ...      0.0     0.0     0.0   0.0    0.0   \n",
       "\n",
       "   \".\"  fonda  bunni  ..........  dupe  \n",
       "0  0.0    0.0    0.0         0.0   0.0  \n",
       "1  0.0    0.0    0.0         0.0   0.0  \n",
       "2  0.0    0.0    0.0         0.0   0.0  \n",
       "3  0.0    0.0    0.0         0.0   0.0  \n",
       "4  0.0    0.0    0.0         0.0   0.0  \n",
       "\n",
       "[5 rows x 5595 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Manual tf-itf\n",
    "\n",
    "def tf_itf(_df):\n",
    "    def idf(column):\n",
    "        return np.log2(1 + len(column) / sum(column > 0))\n",
    "    \n",
    "    _tf = _df.div(_df.sum(axis=1), axis=0)\n",
    "    _tf_idf = (np.log2(1 + _tf)).multiply(_tf.apply(idf))\n",
    "    return _tf_idf\n",
    "\n",
    "tf_idf = tf_itf(bag_of_words)\n",
    "tf_idf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-28T14:41:02.679635Z",
     "start_time": "2018-07-28T14:41:02.535297Z"
    }
   },
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter, OrderedDict\n",
    "import re\n",
    "import string\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "# NLTK imports\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# SKLearn related imports\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('data_for_notes/imdb_sentiment.csv')\n",
    "\n",
    "# Get the text\n",
    "docs = df['text']\n",
    "\n",
    "# Split in train and validation\n",
    "train_df, validation_df = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-28T14:41:02.692111Z",
     "start_time": "2018-07-28T14:41:02.682714Z"
    }
   },
   "outputs": [],
   "source": [
    "## Bag of words with sci-kit pipeline\n",
    "\n",
    "# Custom transformer to implement sentence cleaning\n",
    "class TextCleanerTransformer(TransformerMixin):\n",
    "    def __init__(self, tokenizer, stemmer, regex_list,\n",
    "                 lower=True, remove_punct=True):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stemmer = stemmer\n",
    "        self.regex_list = regex_list\n",
    "        self.lower = lower\n",
    "        self.remove_punct = remove_punct\n",
    "        \n",
    "    def transform(self, X, *_):\n",
    "        X = list(map(self._clean_sentence, X))\n",
    "        return X\n",
    "    \n",
    "    def _clean_sentence(self, sentence):\n",
    "        \n",
    "        # Replace given regexes\n",
    "        for regex in self.regex_list:\n",
    "            sentence = re.sub(regex[0], regex[1], sentence)\n",
    "            \n",
    "        # lowercase\n",
    "        if self.lower:\n",
    "            sentence = sentence.lower()\n",
    "\n",
    "        # Split sentence into list of words\n",
    "        words = self.tokenizer.tokenize(sentence)\n",
    "            \n",
    "        # Remove punctuation\n",
    "        if self.remove_punct:\n",
    "            words = list(filter(lambda x: x not in string.punctuation, words))\n",
    "\n",
    "        # Stem words\n",
    "        if self.stemmer:\n",
    "            words = map(self.stemmer.stem, words)\n",
    "\n",
    "        # Join list elements into string\n",
    "        sentence = \" \".join(words)\n",
    "        \n",
    "        return sentence\n",
    "    \n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just created a class that has a transform() method that will apply the method _clean_sentence() to every sentence of its input X. Note that you can choose the tokenizer and the stemmer as inputs of this class - you can choose which ones you prefer to use. You can also give the class a list of tuples that are regexes that you want to substitute for something in your sentences.\n",
    "\n",
    "Let's use the same tokenizer, stemmer and html regex that we were using before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-28T14:41:02.717781Z",
     "start_time": "2018-07-28T14:41:02.697152Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Initialize a tokenizer and a stemmer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "regex_list = [(\"<[^>]*>\", \"\")\n",
    "             ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline with custom cleanner transformer, CountVectorizer implements the bag of words with adicional n-grams that you can select in the ngram_range, followed by the TfidfTransformer to apply tf-itf and a classifier (in this case multinomial NayveBayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-28T14:41:21.926510Z",
     "start_time": "2018-07-28T14:41:02.722037Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.826"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the pipeline\n",
    "text_clf = Pipeline([('stemm', TextCleanerTransformer(tokenizer, stemmer, regex_list)),\n",
    "                   ('vect', CountVectorizer(stop_words='english', ngram_range=(1,2))),\n",
    "                   ('tfidf', TfidfTransformer()),\n",
    "                   ('clf', MultinomialNB())])\n",
    "# Train the classifier\n",
    "text_clf.fit(map(str, train_df['text'].values), train_df['sentiment'].values)\n",
    "\n",
    "predicted = text_clf.predict(map(str, validation_df['text'].values))\n",
    "np.mean(predicted == validation_df['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing Dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection and truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-28T14:41:21.933803Z",
     "start_time": "2018-07-28T14:41:21.929880Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-28T14:41:21.950331Z",
     "start_time": "2018-07-28T14:41:21.938103Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples 4000\n",
      "Number of words per sample 1347.6965\n",
      "Ratio 2.96802729694705\n"
     ]
    }
   ],
   "source": [
    "# According to goole we should calculate the number of samples divided by the number of words per sample\n",
    "\n",
    "num_samples = train_df.shape[0]\n",
    "print(\"Number of samples\", num_samples)\n",
    "num_words_per_sample = train_df['text'].apply(len).sum()/num_samples\n",
    "print(\"Number of words per sample\", num_words_per_sample)\n",
    "ratio = num_samples/num_words_per_sample\n",
    "print(\"Ratio\", ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reducing by truncating max_features in Tf-itf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-28T14:41:41.512762Z",
     "start_time": "2018-07-28T14:41:21.954912Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.835"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#You can reduce the dimension by passing a max_features to TfitfVectorizer -\n",
    "#which does the job of the previous CountVectorizer and TfitfTransformer\n",
    "#tthis can tipicaly improve accuracy by getting rid of noisy features/n-grams\n",
    "\n",
    "k=5000\n",
    "\n",
    "# Build the pipeline\n",
    "text_clf = Pipeline([('stemm', TextCleanerTransformer(tokenizer, stemmer, regex_list)),\n",
    "                   ('vect', TfidfVectorizer(ngram_range=(1,2), stop_words='english', max_features=k)),\n",
    "                   ('clf', MultinomialNB())])\n",
    "# Train the classifier\n",
    "text_clf.fit(map(str, train_df['text'].values), train_df['sentiment'].values)\n",
    "\n",
    "predicted = text_clf.predict(map(str, validation_df['text'].values))\n",
    "np.mean(predicted == validation_df['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using statistical test and the SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-28T14:42:01.087071Z",
     "start_time": "2018-07-28T14:41:41.517378Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.841"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An alterantive is to use statistical methods such as the chi-square test to select the best features.\n",
    "#These would be words/n-grams that are frequent in one class and not the others for instance.\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "k =5000\n",
    "\n",
    "# Build the pipeline\n",
    "text_clf = Pipeline([('stemm', TextCleanerTransformer(tokenizer, stemmer, regex_list)),\n",
    "                   ('vect', TfidfVectorizer(ngram_range=(1,2), stop_words='english')),\n",
    "                    ('selectKBest', SelectKBest(chi2, k=k)),\n",
    "                   ('clf', MultinomialNB())])\n",
    "\n",
    "# Train the classifier\n",
    "text_clf.fit(map(str, train_df['text'].values), train_df['sentiment'].values)\n",
    "\n",
    "predicted = text_clf.predict(map(str, validation_df['text'].values))\n",
    "np.mean(predicted == validation_df['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-28T14:42:22.150944Z",
     "start_time": "2018-07-28T14:42:01.090714Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can also use f_classif to select the best features - google recommends it!\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "k =5000\n",
    "\n",
    "# Build the pipeline\n",
    "text_clf = Pipeline([('stemm', TextCleanerTransformer(tokenizer, stemmer, regex_list)),\n",
    "                   ('vect', TfidfVectorizer(ngram_range=(1,2), stop_words='english')),\n",
    "                    ('selectKBest', SelectKBest(f_classif, k=k)),\n",
    "                   ('clf', MultinomialNB())])\n",
    "\n",
    "# Train the classifier\n",
    "text_clf.fit(map(str, train_df['text'].values), train_df['sentiment'].values)\n",
    "\n",
    "predicted = text_clf.predict(map(str, validation_df['text'].values))\n",
    "np.mean(predicted == validation_df['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA and SVD - didn't work well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-28T14:42:52.755676Z",
     "start_time": "2018-07-28T14:42:22.153865Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.757"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "n =50\n",
    "seed = 42\n",
    "\n",
    "# Build the pipeline\n",
    "text_clf = Pipeline([('stemm', TextCleanerTransformer(tokenizer, stemmer, regex_list)),\n",
    "                   ('vect', TfidfVectorizer(ngram_range=(1,2), stop_words='english')),\n",
    "                    ('SVD', TruncatedSVD(n_components=n, random_state=seed)),\n",
    "                    ('svd_norm', Normalizer(copy=False)),\n",
    "                   ('clf', KNeighborsClassifier())])\n",
    "\n",
    "# Train the classifier\n",
    "text_clf.fit(map(str, train_df['text'].values), train_df['sentiment'].values)\n",
    "\n",
    "predicted = text_clf.predict(map(str, validation_df['text'].values))\n",
    "np.mean(predicted == validation_df['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-28T14:43:19.858965Z",
     "start_time": "2018-07-28T14:42:52.758649Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.756"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n =10\n",
    "seed = 42\n",
    "## Atempt at making PCA work for this case - DenseTransformer converts sparse matrix from tf-itf to dense\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class DenseTransformer(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "\n",
    "# Build the pipeline\n",
    "text_clf = Pipeline([('stemm', TextCleanerTransformer(tokenizer, stemmer, regex_list)),\n",
    "                   ('vect', TfidfVectorizer(ngram_range=(0,1), stop_words='english')),\n",
    "                      ('to_dense', DenseTransformer()),\n",
    "                    ('PCA', PCA(n_components=n, random_state=seed)),\n",
    "                   ('clf', KNeighborsClassifier())])\n",
    "\n",
    "# Train the classifier\n",
    "text_clf.fit(map(str, train_df['text'].values), train_df['sentiment'].values)\n",
    "\n",
    "predicted = text_clf.predict(map(str, validation_df['text'].values))\n",
    "np.mean(predicted == validation_df['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T09:58:11.506330Z",
     "start_time": "2018-07-27T09:58:11.502098Z"
    }
   },
   "source": [
    "### More advanced pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-28T14:43:19.877037Z",
     "start_time": "2018-07-28T14:43:19.863635Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "X = train_df['text'].apply(str).values\n",
    "print(type(X))\n",
    "y = train_df['sentiment'].map({'Negative' : 0, 'Positive' : 1}).values\n",
    "print(y[33])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-28T14:43:19.896090Z",
     "start_time": "2018-07-28T14:43:19.885773Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline that runs several models in gridsearch cv to select best one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-28T14:43:19.921943Z",
     "start_time": "2018-07-28T14:43:19.903828Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class TextCleanerTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, tokenizer, stemmer, regex_list,\n",
    "                 lower=True, remove_punct=True):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stemmer = stemmer\n",
    "        self.regex_list = regex_list\n",
    "        self.lower = lower\n",
    "        self.remove_punct = remove_punct\n",
    "        \n",
    "    def transform(self, X, *_):\n",
    "        X = list(map(self._clean_sentence, X))\n",
    "        return X\n",
    "    \n",
    "    def _clean_sentence(self, sentence):\n",
    "        \n",
    "        # Replace given regexes\n",
    "        for regex in self.regex_list:\n",
    "            sentence = re.sub(regex[0], regex[1], sentence)\n",
    "            \n",
    "        # lowercase\n",
    "        if self.lower:\n",
    "            sentence = sentence.lower()\n",
    "\n",
    "        # Split sentence into list of words\n",
    "        words = self.tokenizer.tokenize(sentence)\n",
    "            \n",
    "        # Remove punctuation\n",
    "        if self.remove_punct:\n",
    "            words = list(filter(lambda x: x not in string.punctuation, words))\n",
    "\n",
    "        # Stem words\n",
    "        if self.stemmer:\n",
    "            words = map(self.stemmer.stem, words)\n",
    "\n",
    "        # Join list elements into string\n",
    "        sentence = \" \".join(words)\n",
    "        \n",
    "        return sentence\n",
    "    \n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "    \n",
    "    #def __deepcopy__(self, memo):\n",
    "     #   return TextCleanerTransformer(self.tokenizer, self.stemmer, self.regex_list, self.lower, self.remove_punct)\n",
    "    \n",
    "    #def __deepcopy__(self, memo):\n",
    "    #    return TextCleanerTransformer(copy.deepcopy(dict(self)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-28T14:46:14.037958Z",
     "start_time": "2018-07-28T14:43:19.926575Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "Grid search cv scores: [0.845   0.83675 0.5815  0.8305  0.8145  0.5155 ]\n",
      "Acuracy on validation: 0.839\n"
     ]
    }
   ],
   "source": [
    "#trying to combine a pipeline with a grid search to select model and model parameters\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "\n",
    "\n",
    "import copy\n",
    "\n",
    "\n",
    "\n",
    "search_space = [{'classifier': [LogisticRegression()],\n",
    "                 'classifier__penalty': ['l1', 'l2']},\n",
    "                {'classifier': [RandomForestClassifier()],\n",
    "                 'classifier__n_estimators': [10, 15],\n",
    "                 'classifier__max_features': [ 2, 3]}]\n",
    "\n",
    "search_space2 = [{'classifier': [MultinomialNB()]},\n",
    "                {'classifier': [RandomForestClassifier()],\n",
    "                 'classifier__n_estimators': [1000, 5000],\n",
    "                 'classifier__max_features': [ 2, 3]}]\n",
    "\n",
    "loads_of_classifiers = [{'classifier': [MultinomialNB()]},\n",
    "                        {'classifier': [RandomForestClassifier()],\n",
    "                         'classifier__n_estimators': [2000]},\n",
    "                        {'classifier': [KNeighborsClassifier()]},\n",
    "                        {'classifier': [LogisticRegression()]},\n",
    "                        {'classifier': [XGBClassifier()],\n",
    "                        'classifier__n_estimators': [2000]},\n",
    "                       {'classifier': [LinearSVC()]},]\n",
    "\n",
    "\n",
    "search_space3 = [{'n_estimators': [10, 100, 1000],\n",
    "                'max_features': [1, 2, 3]}]\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "k =5000\n",
    "\n",
    "# Build the pipeline\n",
    "pipe = Pipeline([#('stemm', TextCleanerTransformer(tokenizer, stemmer, regex_list)),\n",
    "                   ('vect', TfidfVectorizer(ngram_range=(1,2), stop_words='english')),\n",
    "                    ('selectKBest', SelectKBest(f_classif, k=k)),\n",
    "                   ('classifier', RandomForestClassifier())])\n",
    "\n",
    "\n",
    "clf = GridSearchCV(pipe, loads_of_classifiers, cv=5, verbose=0, n_jobs=6)\n",
    "# Train the classifier\n",
    "\n",
    "X = train_df['text'].apply(str).values\n",
    "y =  train_df['sentiment'].map({'Negative' : 0, 'Positive' : 1}).values\n",
    "X_val = validation_df['text'].apply(str).values\n",
    "y_val = validation_df['sentiment'].map({'Negative' : 0, 'Positive' : 1}).values\n",
    "\n",
    "\n",
    "best_model = clf.fit(X, y)\n",
    "print(best_model.best_estimator_.get_params()['classifier'])\n",
    "\n",
    "cv_score = clf.cv_results_['mean_test_score']\n",
    "\n",
    "print(\"Grid search cv scores:\", cv_score)\n",
    "\n",
    "predicted = best_model.predict(X_val)\n",
    "print(\"Acuracy on validation:\", np.mean(predicted == y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-28T14:46:14.100356Z",
     "start_time": "2018-07-28T14:46:14.041360Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_classifier</th>\n",
       "      <th>param_classifier__n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>...</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.159618</td>\n",
       "      <td>0.681536</td>\n",
       "      <td>0.84500</td>\n",
       "      <td>0.927999</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'classifier': MultinomialNB(alpha=1.0, class_...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.846442</td>\n",
       "      <td>0.924039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.82875</td>\n",
       "      <td>0.933750</td>\n",
       "      <td>0.843554</td>\n",
       "      <td>0.929397</td>\n",
       "      <td>0.849812</td>\n",
       "      <td>0.929709</td>\n",
       "      <td>0.041566</td>\n",
       "      <td>0.020795</td>\n",
       "      <td>0.009189</td>\n",
       "      <td>0.003940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>74.226024</td>\n",
       "      <td>1.831689</td>\n",
       "      <td>0.83675</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>RandomForestClassifier(bootstrap=True, class_w...</td>\n",
       "      <td>2000</td>\n",
       "      <td>{'classifier': RandomForestClassifier(bootstra...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.826467</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.82375</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.843554</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.847309</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.868859</td>\n",
       "      <td>0.037151</td>\n",
       "      <td>0.009673</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.736179</td>\n",
       "      <td>0.573079</td>\n",
       "      <td>0.83050</td>\n",
       "      <td>0.882688</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'classifier': LogisticRegression(C=1.0, class...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.830212</td>\n",
       "      <td>0.887465</td>\n",
       "      <td>...</td>\n",
       "      <td>0.81250</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.879413</td>\n",
       "      <td>0.833542</td>\n",
       "      <td>0.886286</td>\n",
       "      <td>0.087899</td>\n",
       "      <td>0.029120</td>\n",
       "      <td>0.013226</td>\n",
       "      <td>0.003451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58.332686</td>\n",
       "      <td>0.621278</td>\n",
       "      <td>0.81450</td>\n",
       "      <td>0.999625</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>2000</td>\n",
       "      <td>{'classifier': XGBClassifier(base_score=0.5, b...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.818976</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.80750</td>\n",
       "      <td>0.999375</td>\n",
       "      <td>0.809762</td>\n",
       "      <td>0.999688</td>\n",
       "      <td>0.816020</td>\n",
       "      <td>0.999688</td>\n",
       "      <td>2.348628</td>\n",
       "      <td>0.034328</td>\n",
       "      <td>0.005032</td>\n",
       "      <td>0.000234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.250758</td>\n",
       "      <td>0.709110</td>\n",
       "      <td>0.58150</td>\n",
       "      <td>0.662296</td>\n",
       "      <td>KNeighborsClassifier(algorithm='auto', leaf_si...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'classifier': KNeighborsClassifier(algorithm=...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.569288</td>\n",
       "      <td>0.549234</td>\n",
       "      <td>...</td>\n",
       "      <td>0.56875</td>\n",
       "      <td>0.661875</td>\n",
       "      <td>0.579474</td>\n",
       "      <td>0.639488</td>\n",
       "      <td>0.591990</td>\n",
       "      <td>0.815683</td>\n",
       "      <td>0.318576</td>\n",
       "      <td>0.070633</td>\n",
       "      <td>0.011820</td>\n",
       "      <td>0.086166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10.847028</td>\n",
       "      <td>1.805750</td>\n",
       "      <td>0.51550</td>\n",
       "      <td>0.515500</td>\n",
       "      <td>SVC(C=1.0, cache_size=200, class_weight=None, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'classifier': SVC(C=1.0, cache_size=200, clas...</td>\n",
       "      <td>6</td>\n",
       "      <td>0.515605</td>\n",
       "      <td>0.515474</td>\n",
       "      <td>...</td>\n",
       "      <td>0.51500</td>\n",
       "      <td>0.515625</td>\n",
       "      <td>0.515645</td>\n",
       "      <td>0.515464</td>\n",
       "      <td>0.515645</td>\n",
       "      <td>0.515464</td>\n",
       "      <td>0.932571</td>\n",
       "      <td>0.306263</td>\n",
       "      <td>0.000251</td>\n",
       "      <td>0.000063</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0       7.159618         0.681536          0.84500          0.927999   \n",
       "1      74.226024         1.831689          0.83675          1.000000   \n",
       "3       5.736179         0.573079          0.83050          0.882688   \n",
       "4      58.332686         0.621278          0.81450          0.999625   \n",
       "2       5.250758         0.709110          0.58150          0.662296   \n",
       "5      10.847028         1.805750          0.51550          0.515500   \n",
       "\n",
       "                                    param_classifier  \\\n",
       "0  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "1  RandomForestClassifier(bootstrap=True, class_w...   \n",
       "3  LogisticRegression(C=1.0, class_weight=None, d...   \n",
       "4  XGBClassifier(base_score=0.5, booster='gbtree'...   \n",
       "2  KNeighborsClassifier(algorithm='auto', leaf_si...   \n",
       "5  SVC(C=1.0, cache_size=200, class_weight=None, ...   \n",
       "\n",
       "  param_classifier__n_estimators  \\\n",
       "0                            NaN   \n",
       "1                           2000   \n",
       "3                            NaN   \n",
       "4                           2000   \n",
       "2                            NaN   \n",
       "5                            NaN   \n",
       "\n",
       "                                              params  rank_test_score  \\\n",
       "0  {'classifier': MultinomialNB(alpha=1.0, class_...                1   \n",
       "1  {'classifier': RandomForestClassifier(bootstra...                2   \n",
       "3  {'classifier': LogisticRegression(C=1.0, class...                3   \n",
       "4  {'classifier': XGBClassifier(base_score=0.5, b...                4   \n",
       "2  {'classifier': KNeighborsClassifier(algorithm=...                5   \n",
       "5  {'classifier': SVC(C=1.0, cache_size=200, clas...                6   \n",
       "\n",
       "   split0_test_score  split0_train_score       ...         split2_test_score  \\\n",
       "0           0.846442            0.924039       ...                   0.82875   \n",
       "1           0.826467            1.000000       ...                   0.82375   \n",
       "3           0.830212            0.887465       ...                   0.81250   \n",
       "4           0.818976            1.000000       ...                   0.80750   \n",
       "2           0.569288            0.549234       ...                   0.56875   \n",
       "5           0.515605            0.515474       ...                   0.51500   \n",
       "\n",
       "   split2_train_score  split3_test_score  split3_train_score  \\\n",
       "0            0.933750           0.843554            0.929397   \n",
       "1            1.000000           0.843554            1.000000   \n",
       "3            0.880000           0.823529            0.879413   \n",
       "4            0.999375           0.809762            0.999688   \n",
       "2            0.661875           0.579474            0.639488   \n",
       "5            0.515625           0.515645            0.515464   \n",
       "\n",
       "   split4_test_score  split4_train_score  std_fit_time  std_score_time  \\\n",
       "0           0.849812            0.929709      0.041566        0.020795   \n",
       "1           0.847309            1.000000      0.868859        0.037151   \n",
       "3           0.833542            0.886286      0.087899        0.029120   \n",
       "4           0.816020            0.999688      2.348628        0.034328   \n",
       "2           0.591990            0.815683      0.318576        0.070633   \n",
       "5           0.515645            0.515464      0.932571        0.306263   \n",
       "\n",
       "   std_test_score  std_train_score  \n",
       "0        0.009189         0.003940  \n",
       "1        0.009673         0.000000  \n",
       "3        0.013226         0.003451  \n",
       "4        0.005032         0.000234  \n",
       "2        0.011820         0.086166  \n",
       "5        0.000251         0.000063  \n",
       "\n",
       "[6 rows x 22 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_cv_summary(grid_clf):\n",
    "    return pd.DataFrame(clf.cv_results_).sort_values(by='rank_test_score')\n",
    "\n",
    "scores_df = get_cv_summary(clf)\n",
    "scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T08:46:04.469949Z",
     "start_time": "2018-07-27T08:46:03.985129Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-28T14:46:14.385919Z",
     "start_time": "2018-07-28T14:46:14.104665Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-28T14:46:14.393044Z",
     "start_time": "2018-07-28T14:46:14.388930Z"
    }
   },
   "outputs": [],
   "source": [
    "#downloading default model - its the sm model!\n",
    "#!python -m spacy download en\n",
    "#medium model:\n",
    "#!python -m spacy download en_core_web_md\n",
    "#large model\n",
    "#!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-28T14:46:23.725543Z",
     "start_time": "2018-07-28T14:46:14.397026Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-28T14:46:23.759704Z",
     "start_time": "2018-07-28T14:46:23.728239Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_for_notes/spam.csv', encoding='latin1')\n",
    "df.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1,inplace=True)\n",
    "df.rename(columns={\"v1\":\"label\", \"v2\":\"message\"},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating adicional features with spacy POS and NER "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to create adiitonal features - adapt as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-28T14:46:23.770457Z",
     "start_time": "2018-07-28T14:46:23.762547Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_aditional_POS_features(_df, text_column_name,_nlp, return_only_new_feats = False):\n",
    "    n_adj, n_verbs, len_message= [], [], []\n",
    "    _df_copy = _df.copy()\n",
    "   \n",
    "    for doc in _nlp.pipe(_df_copy[text_column_name]):\n",
    "        n_adj.append(len([token for token in doc if token.pos_ == 'ADJ']))\n",
    "        n_verbs.append(len([token for token in doc if token.pos_ == 'VERB']))\n",
    "        len_message.append(len(doc))\n",
    "    _df_copy['n_adj'] = pd.Series(n_adj)\n",
    "    _df_copy['n_verbs'] = pd.Series(n_verbs)\n",
    "    _df_copy['len_text'] = pd.Series(len_message)\n",
    "    \n",
    "    if return_only_new_feats:\n",
    "        return _df_copy.drop(text_column_name, axis=1)\n",
    "    else:\n",
    "        return _df_copy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-28T14:46:55.504691Z",
     "start_time": "2018-07-28T14:46:23.776165Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>n_adj</th>\n",
       "      <th>n_verbs</th>\n",
       "      <th>len_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  n_adj  n_verbs  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...      4        2   \n",
       "1   ham                      Ok lar... Joking wif u oni...      1        1   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...      2        4   \n",
       "3   ham  U dun say so early hor... U c already then say...      1        2   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      0        4   \n",
       "\n",
       "   len_text  \n",
       "0        24  \n",
       "1         8  \n",
       "2        31  \n",
       "3        13  \n",
       "4        15  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_add = create_aditional_POS_features(df, \"message\",nlp)\n",
    "df_add.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-28T14:46:55.519942Z",
     "start_time": "2018-07-28T14:46:55.509050Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(df_add, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text and number column selectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-28T14:46:55.562112Z",
     "start_time": "2018-07-28T14:46:55.524065Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class TextSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on text columns in the data\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[self.key]\n",
    "    \n",
    "class NumberSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on numeric columns in the data\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[[self.key]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline with feature union to combine features and simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-28T14:46:56.112028Z",
     "start_time": "2018-07-28T14:46:55.564398Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9632\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "text = Pipeline([\n",
    "                ('selector', TextSelector(key='message')),\n",
    "                ('tfidf', TfidfVectorizer(ngram_range=(1,2), stop_words='english'))\n",
    "            ])\n",
    "\n",
    "adj =  Pipeline([\n",
    "                ('selector', NumberSelector(key='n_adj')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "\n",
    "verbs =  Pipeline([\n",
    "                ('selector', NumberSelector(key='n_verbs')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "\n",
    "len_text =  Pipeline([\n",
    "                ('selector', NumberSelector(key='len_text')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "\n",
    "feats = FeatureUnion([('text', text), \n",
    "                      ('adj', adj),\n",
    "                      ('verbs', verbs),\n",
    "                      ('len_text', len_text)\n",
    "                       ])\n",
    "\n",
    "\n",
    "final_pipe = Pipeline([('features',feats),\n",
    "                     ('classifier', RandomForestClassifier())])\n",
    "\n",
    "#You can pass the label to the classifier because of the feature selectors\n",
    "\n",
    "X = train_data.drop('label', axis=1)\n",
    "y =  train_data.label\n",
    "X_val = test_data\n",
    "y_val = test_data.label\n",
    "\n",
    "final_pipe.fit(X, y)\n",
    "\n",
    "preds = final_pipe.predict(X_val)\n",
    "accuracy = np.mean(preds == y_val)\n",
    "\n",
    "print(\"Accuracy: {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-28T11:04:43.055219Z",
     "start_time": "2018-07-28T11:04:43.050943Z"
    }
   },
   "source": [
    "#### Pipeline with feature union but also gridsearchCv to select best model from large list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-28T15:27:50.264617Z",
     "start_time": "2018-07-28T15:26:41.106750Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done  35 out of  35 | elapsed:  1.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Grid search cv scores: [0.96903747 0.89118241 0.95759479 0.9715055  0.9860893  0.9867624\n",
      " 0.97307606]\n",
      "Acuracy on validation: 0.9811659192825112\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "\n",
    "text = Pipeline([\n",
    "                ('selector', TextSelector(key='message')),\n",
    "                ('tfidf', TfidfVectorizer(ngram_range=(1,2), stop_words='english'))\n",
    "            ])\n",
    "\n",
    "adj =  Pipeline([\n",
    "                ('selector', NumberSelector(key='n_adj')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "\n",
    "verbs =  Pipeline([\n",
    "                ('selector', NumberSelector(key='n_verbs')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "\n",
    "len_text =  Pipeline([\n",
    "                ('selector', NumberSelector(key='len_text')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "\n",
    "feats = FeatureUnion([('text', text), \n",
    "                      ('adj', adj),\n",
    "                      ('verbs', verbs),\n",
    "                      ('len_text', len_text)\n",
    "                       ])\n",
    "\n",
    "\n",
    "final_pipe = Pipeline([('features',feats),\n",
    "                       ('selectKBest', SelectKBest(f_classif, k=20000)),\n",
    "                     ('classifier', RandomForestClassifier())])\n",
    "\n",
    "\n",
    "loads_of_classifiers = [ {'classifier': [RandomForestClassifier()],\n",
    "                         'classifier__n_estimators': [2000]},\n",
    "                        {'classifier': [KNeighborsClassifier()]},\n",
    "                        {'classifier': [LogisticRegression()]},\n",
    "                        {'classifier': [XGBClassifier()],\n",
    "                        'classifier__n_estimators': [1000]},\n",
    "                       {'classifier': [LinearSVC()]},\n",
    "                         {'classifier': [SVC(kernel='linear', probability=True)]},\n",
    "                       {'classifier': [ExtraTreesClassifier()]},]\n",
    "\n",
    "one_classifier = [{'classifier': [RandomForestClassifier()]},\n",
    "                        ]\n",
    "\n",
    "#You can pass the label to the classifier because of the feature selectors\n",
    "\n",
    "X = train_data.drop('label', axis=1)\n",
    "y =  train_data.label.map({'ham' : 0, 'spam' : 1})\n",
    "X_val = test_data\n",
    "y_val = test_data.label.map({'ham' : 0, 'spam' : 1})\n",
    "\n",
    "\n",
    "clf = GridSearchCV(final_pipe, loads_of_classifiers, cv=5, verbose=1, n_jobs=6)\n",
    "\n",
    "best_model = clf.fit(X, y)\n",
    "print(best_model.best_estimator_.get_params()['classifier'])\n",
    "\n",
    "cv_score = clf.cv_results_['mean_test_score']\n",
    "\n",
    "print(\"Grid search cv scores:\", cv_score)\n",
    "\n",
    "predicted = best_model.predict(X_val)\n",
    "print(\"Acuracy on validation:\", np.mean(predicted == y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-28T15:31:37.851344Z",
     "start_time": "2018-07-28T15:31:37.795288Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_classifier</th>\n",
       "      <th>param_classifier__n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>...</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.630916</td>\n",
       "      <td>0.200715</td>\n",
       "      <td>0.986762</td>\n",
       "      <td>0.998542</td>\n",
       "      <td>SVC(C=1.0, cache_size=200, class_weight=None, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'classifier': SVC(C=1.0, cache_size=200, clas...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.984305</td>\n",
       "      <td>0.998878</td>\n",
       "      <td>...</td>\n",
       "      <td>0.987654</td>\n",
       "      <td>0.998598</td>\n",
       "      <td>0.991021</td>\n",
       "      <td>0.998317</td>\n",
       "      <td>0.978676</td>\n",
       "      <td>0.998598</td>\n",
       "      <td>0.307553</td>\n",
       "      <td>0.020943</td>\n",
       "      <td>0.004887</td>\n",
       "      <td>0.000210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.868570</td>\n",
       "      <td>0.067414</td>\n",
       "      <td>0.986089</td>\n",
       "      <td>0.999832</td>\n",
       "      <td>LinearSVC(C=1.0, class_weight=None, dual=True,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'classifier': LinearSVC(C=1.0, class_weight=N...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.984305</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.985410</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.991021</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.978676</td>\n",
       "      <td>0.999439</td>\n",
       "      <td>0.054041</td>\n",
       "      <td>0.008714</td>\n",
       "      <td>0.004633</td>\n",
       "      <td>0.000224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.593773</td>\n",
       "      <td>0.070691</td>\n",
       "      <td>0.973076</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>ExtraTreesClassifier(bootstrap=False, class_we...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'classifier': ExtraTreesClassifier(bootstrap=...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.978700</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.975309</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966330</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.975309</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.035734</td>\n",
       "      <td>0.007085</td>\n",
       "      <td>0.004434</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.581615</td>\n",
       "      <td>0.117387</td>\n",
       "      <td>0.971505</td>\n",
       "      <td>0.994223</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'classifier': XGBClassifier(base_score=0.5, b...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.974215</td>\n",
       "      <td>0.995231</td>\n",
       "      <td>...</td>\n",
       "      <td>0.973064</td>\n",
       "      <td>0.992709</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.994952</td>\n",
       "      <td>0.967452</td>\n",
       "      <td>0.994672</td>\n",
       "      <td>0.413712</td>\n",
       "      <td>0.019491</td>\n",
       "      <td>0.005802</td>\n",
       "      <td>0.000948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38.496927</td>\n",
       "      <td>1.489448</td>\n",
       "      <td>0.969037</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>RandomForestClassifier(bootstrap=True, class_w...</td>\n",
       "      <td>2000</td>\n",
       "      <td>{'classifier': RandomForestClassifier(bootstra...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.974215</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.974186</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.964085</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966330</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.841346</td>\n",
       "      <td>0.055488</td>\n",
       "      <td>0.004296</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.435610</td>\n",
       "      <td>0.065619</td>\n",
       "      <td>0.957595</td>\n",
       "      <td>0.957707</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'classifier': LogisticRegression(C=1.0, class...</td>\n",
       "      <td>6</td>\n",
       "      <td>0.958520</td>\n",
       "      <td>0.956241</td>\n",
       "      <td>...</td>\n",
       "      <td>0.961841</td>\n",
       "      <td>0.958497</td>\n",
       "      <td>0.947250</td>\n",
       "      <td>0.959338</td>\n",
       "      <td>0.952862</td>\n",
       "      <td>0.957936</td>\n",
       "      <td>0.027535</td>\n",
       "      <td>0.005595</td>\n",
       "      <td>0.007017</td>\n",
       "      <td>0.001174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.345072</td>\n",
       "      <td>0.191152</td>\n",
       "      <td>0.891182</td>\n",
       "      <td>0.905822</td>\n",
       "      <td>KNeighborsClassifier(algorithm='auto', leaf_si...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'classifier': KNeighborsClassifier(algorithm=...</td>\n",
       "      <td>7</td>\n",
       "      <td>0.893498</td>\n",
       "      <td>0.906592</td>\n",
       "      <td>...</td>\n",
       "      <td>0.890011</td>\n",
       "      <td>0.902131</td>\n",
       "      <td>0.891134</td>\n",
       "      <td>0.908020</td>\n",
       "      <td>0.892256</td>\n",
       "      <td>0.906898</td>\n",
       "      <td>0.045761</td>\n",
       "      <td>0.017954</td>\n",
       "      <td>0.001587</td>\n",
       "      <td>0.002016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "5       6.630916         0.200715         0.986762          0.998542   \n",
       "4       0.868570         0.067414         0.986089          0.999832   \n",
       "6       0.593773         0.070691         0.973076          1.000000   \n",
       "3      16.581615         0.117387         0.971505          0.994223   \n",
       "0      38.496927         1.489448         0.969037          1.000000   \n",
       "2       0.435610         0.065619         0.957595          0.957707   \n",
       "1       0.345072         0.191152         0.891182          0.905822   \n",
       "\n",
       "                                    param_classifier  \\\n",
       "5  SVC(C=1.0, cache_size=200, class_weight=None, ...   \n",
       "4  LinearSVC(C=1.0, class_weight=None, dual=True,...   \n",
       "6  ExtraTreesClassifier(bootstrap=False, class_we...   \n",
       "3  XGBClassifier(base_score=0.5, booster='gbtree'...   \n",
       "0  RandomForestClassifier(bootstrap=True, class_w...   \n",
       "2  LogisticRegression(C=1.0, class_weight=None, d...   \n",
       "1  KNeighborsClassifier(algorithm='auto', leaf_si...   \n",
       "\n",
       "  param_classifier__n_estimators  \\\n",
       "5                            NaN   \n",
       "4                            NaN   \n",
       "6                            NaN   \n",
       "3                           1000   \n",
       "0                           2000   \n",
       "2                            NaN   \n",
       "1                            NaN   \n",
       "\n",
       "                                              params  rank_test_score  \\\n",
       "5  {'classifier': SVC(C=1.0, cache_size=200, clas...                1   \n",
       "4  {'classifier': LinearSVC(C=1.0, class_weight=N...                2   \n",
       "6  {'classifier': ExtraTreesClassifier(bootstrap=...                3   \n",
       "3  {'classifier': XGBClassifier(base_score=0.5, b...                4   \n",
       "0  {'classifier': RandomForestClassifier(bootstra...                5   \n",
       "2  {'classifier': LogisticRegression(C=1.0, class...                6   \n",
       "1  {'classifier': KNeighborsClassifier(algorithm=...                7   \n",
       "\n",
       "   split0_test_score  split0_train_score       ...         split2_test_score  \\\n",
       "5           0.984305            0.998878       ...                  0.987654   \n",
       "4           0.984305            1.000000       ...                  0.985410   \n",
       "6           0.978700            1.000000       ...                  0.975309   \n",
       "3           0.974215            0.995231       ...                  0.973064   \n",
       "0           0.974215            1.000000       ...                  0.974186   \n",
       "2           0.958520            0.956241       ...                  0.961841   \n",
       "1           0.893498            0.906592       ...                  0.890011   \n",
       "\n",
       "   split2_train_score  split3_test_score  split3_train_score  \\\n",
       "5            0.998598           0.991021            0.998317   \n",
       "4            1.000000           0.991021            1.000000   \n",
       "6            1.000000           0.966330            1.000000   \n",
       "3            0.992709           0.962963            0.994952   \n",
       "0            1.000000           0.964085            1.000000   \n",
       "2            0.958497           0.947250            0.959338   \n",
       "1            0.902131           0.891134            0.908020   \n",
       "\n",
       "   split4_test_score  split4_train_score  std_fit_time  std_score_time  \\\n",
       "5           0.978676            0.998598      0.307553        0.020943   \n",
       "4           0.978676            0.999439      0.054041        0.008714   \n",
       "6           0.975309            1.000000      0.035734        0.007085   \n",
       "3           0.967452            0.994672      0.413712        0.019491   \n",
       "0           0.966330            1.000000      0.841346        0.055488   \n",
       "2           0.952862            0.957936      0.027535        0.005595   \n",
       "1           0.892256            0.906898      0.045761        0.017954   \n",
       "\n",
       "   std_test_score  std_train_score  \n",
       "5        0.004887         0.000210  \n",
       "4        0.004633         0.000224  \n",
       "6        0.004434         0.000000  \n",
       "3        0.005802         0.000948  \n",
       "0        0.004296         0.000000  \n",
       "2        0.007017         0.001174  \n",
       "1        0.001587         0.002016  \n",
       "\n",
       "[7 rows x 22 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df = get_cv_summary(clf)\n",
    "scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline for a ensemble classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-28T15:43:28.962899Z",
     "start_time": "2018-07-28T15:43:28.950809Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import  ExtraTreesClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "text = Pipeline([\n",
    "                ('selector', TextSelector(key='message')),\n",
    "                ('tfidf', TfidfVectorizer(ngram_range=(1,2), stop_words='english'))\n",
    "            ])\n",
    "\n",
    "adj =  Pipeline([\n",
    "                ('selector', NumberSelector(key='n_adj')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "\n",
    "verbs =  Pipeline([\n",
    "                ('selector', NumberSelector(key='n_verbs')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "\n",
    "len_text =  Pipeline([\n",
    "                ('selector', NumberSelector(key='len_text')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "\n",
    "feats = FeatureUnion([('text', text), \n",
    "                      ('adj', adj),\n",
    "                      ('verbs', verbs),\n",
    "                      ('len_text', len_text)\n",
    "                       ])\n",
    "\n",
    "\n",
    "clf1 = Pipeline([('features',feats),\n",
    "                 ('classifier', RandomForestClassifier(n_estimators=2000))])\n",
    "clf2 = Pipeline([('features',feats),\n",
    "                 ('classifier', LogisticRegression())])\n",
    "clf3 = Pipeline([('features',feats),\n",
    "                 ('classifier', XGBClassifier(n_estimators=2000))])\n",
    "clf4 = Pipeline([('features',feats),\n",
    "                 ('classifier', SVC(kernel='linear', probability=True))])\n",
    "clf5 = Pipeline([('features',feats),\n",
    "                 ('classifier', ExtraTreesClassifier())])\n",
    "clf6 = Pipeline([('features',feats),\n",
    "                 ('classifier', MultinomialNB())])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-28T15:44:48.847736Z",
     "start_time": "2018-07-28T15:43:32.762676Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf1 : 0.9722\n",
      "clf2 : 0.9471\n",
      "clf3 : 0.9641\n",
      "clf4 : 0.9812\n",
      "clf5 : 0.9740\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X must be non-negative",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-1da9d53aaf37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mest_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbase_learners\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mest_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    602\u001b[0m         self.feature_count_ = np.zeros((n_effective_classes, n_features),\n\u001b[1;32m    603\u001b[0m                                        dtype=np.float64)\n\u001b[0;32m--> 604\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    605\u001b[0m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_feature_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_count\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    706\u001b[0m         \u001b[0;34m\"\"\"Count and smooth feature occurrences.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input X must be non-negative\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_count_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_count_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input X must be non-negative"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "base_learners = [\n",
    "    (\"clf1\",clf1), (\"clf2\",clf2), (\"clf3\",clf3), (\"clf4\",clf4), (\"clf5\",clf5)\n",
    "]\n",
    "\n",
    "P = np.zeros((X_val.shape[0], len(base_learners)))\n",
    "P = pd.DataFrame(P, columns=[e for e, _ in base_learners])\n",
    "\n",
    "for est_name, est in base_learners:\n",
    "    est.fit(X, y)\n",
    "    p = est.predict(X_val)\n",
    "    P.loc[:, est_name] = p\n",
    "    print(\"%3s : %.4f\" % (est_name, np.mean(p == y_val)))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-28T15:34:08.315275Z",
     "start_time": "2018-07-28T15:34:07.998339Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f7bbe8a4940>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(P.corr(), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-28T15:35:27.969789Z",
     "start_time": "2018-07-28T15:34:11.848192Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuracy on validation: 0.97847533632287\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "voting = VotingClassifier(estimators=[(\"clf1\",clf1), (\"clf2\",clf2), (\"clf3\",clf3), (\"clf4\",clf4), (\"clf5\",clf5)], \n",
    "                          voting='soft')\n",
    "\n",
    "best_model = voting.fit(X, y)\n",
    "\n",
    "predicted = best_model.predict(X_val)\n",
    "print(\"Acuracy on validation:\", np.mean(predicted == y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-28T15:35:33.450568Z",
     "start_time": "2018-07-28T15:35:31.521228Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9593476 , 0.04065239],\n",
       "       [0.85387962, 0.14612038],\n",
       "       [0.65394953, 0.34605047],\n",
       "       ...,\n",
       "       [0.99206882, 0.00793118],\n",
       "       [0.99167532, 0.00832468],\n",
       "       [0.5405378 , 0.4594622 ]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.predict_proba(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MultiClass\n",
    "\n",
    "Check this link and adapt code:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/multiclass.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt at using vector representation of words and documents with spacy - fail in the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-28T14:48:04.197477Z",
     "start_time": "2018-07-28T14:40:55.896Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_spacy_word_to_vec_representation(_df, text_column_name,_nlp):\n",
    "    train_vec = []\n",
    "    for doc in _nlp.pipe(_df[text_column_name], batch_size=500):\n",
    "        if doc.has_vector:\n",
    "            train_vec.append(doc.vector)\n",
    "        # If doc doesn't have a vector, then fill it with zeros.\n",
    "        else:\n",
    "            train_vec.append(np.zeros((300,), dtype=\"float32\"))\n",
    "        \n",
    "\n",
    "    train_vec = np.array(train_vec)\n",
    "    return pd.DataFrame(train_vec)\n",
    "   \n",
    "    \n",
    "df_vector = create_spacy_word_to_vec_representation(df, \"message\", nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-28T14:48:04.199097Z",
     "start_time": "2018-07-28T14:40:55.902Z"
    }
   },
   "outputs": [],
   "source": [
    "df_vector.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-28T14:48:04.200754Z",
     "start_time": "2018-07-28T14:40:55.907Z"
    }
   },
   "outputs": [],
   "source": [
    "df_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-28T14:48:04.202650Z",
     "start_time": "2018-07-28T14:40:55.913Z"
    }
   },
   "outputs": [],
   "source": [
    "df['message'].loc[44]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-28T14:48:04.203784Z",
     "start_time": "2018-07-28T14:40:55.919Z"
    }
   },
   "outputs": [],
   "source": [
    "# trying to make a transformer\n",
    "class SpacyWord2VecVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, _nlp):\n",
    "        self._nlp = _nlp\n",
    "        \n",
    "    def transform(self, X, *_):\n",
    "        train_vec = []\n",
    "        for doc in self._nlp.pipe(X, batch_size=500):\n",
    "            if doc.has_vector:\n",
    "                train_vec.append(doc.vector)\n",
    "            # If doc doesn't have a vector, then fill it with zeros.\n",
    "            else:\n",
    "                train_vec.append(np.zeros((300,), dtype=\"float32\"))\n",
    "        train_vec = np.array(train_vec)\n",
    "        return train_vec\n",
    "    \n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "\n",
    "spacy_vec = SpacyWord2VecVectorizer(nlp)\n",
    "temp = spacy_vec.transform(df['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-28T14:48:04.204909Z",
     "start_time": "2018-07-28T14:40:55.926Z"
    }
   },
   "outputs": [],
   "source": [
    "#gives an error of either memory or other things - gave up on this - cell is disabled with if statmment\n",
    "\n",
    "if 1==0:\n",
    "\n",
    "    from sklearn.pipeline import FeatureUnion\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    train_data, test_data = train_test_split(df_add, test_size=0.2, random_state=42)\n",
    "\n",
    "    text = Pipeline([\n",
    "                    ('selector', TextSelector(key='message')),\n",
    "                    ('spacy_vec', SpacyWord2VecVectorizer(nlp))\n",
    "                ])\n",
    "\n",
    "    adj =  Pipeline([\n",
    "                    ('selector', NumberSelector(key='n_adj')),\n",
    "                    ('standard', StandardScaler())\n",
    "                ])\n",
    "\n",
    "    verbs =  Pipeline([\n",
    "                    ('selector', NumberSelector(key='n_verbs')),\n",
    "                    ('standard', StandardScaler())\n",
    "                ])\n",
    "\n",
    "    len_text =  Pipeline([\n",
    "                    ('selector', NumberSelector(key='len_text')),\n",
    "                    ('standard', StandardScaler())\n",
    "                ])\n",
    "\n",
    "    feats = FeatureUnion([('text', text), \n",
    "                          ('adj', adj),\n",
    "                          ('verbs', verbs),\n",
    "                          ('len_text', len_text)\n",
    "                           ])\n",
    "\n",
    "\n",
    "    final_pipe = Pipeline([('features',feats),\n",
    "                         ('classifier', RandomForestClassifier())])\n",
    "\n",
    "\n",
    "    loads_of_classifiers = [ {'classifier': [RandomForestClassifier()],\n",
    "                             'classifier__n_estimators': [1000]},\n",
    "                            {'classifier': [KNeighborsClassifier()]},\n",
    "                            {'classifier': [LogisticRegression()]},\n",
    "                            {'classifier': [XGBClassifier()],\n",
    "                            'classifier__n_estimators': [1000]},\n",
    "                           {'classifier': [SVC()]},]\n",
    "\n",
    "    one_classifier = [{'classifier': [RandomForestClassifier()]},\n",
    "                            ]\n",
    "\n",
    "    #You can pass the label to the classifier because of the feature selectors\n",
    "\n",
    "    X = train_data.drop('label', axis=1)\n",
    "    y =  train_data.label.map({'ham' : 0, 'spam' : 1})\n",
    "    X_val = test_data\n",
    "    y_val = test_data.label.map({'ham' : 0, 'spam' : 1})\n",
    "\n",
    "\n",
    "    clf = GridSearchCV(final_pipe, one_classifier, cv=5, verbose=1, n_jobs=-1)\n",
    "\n",
    "    best_model = clf.fit(X, y)\n",
    "    print(best_model.best_estimator_.get_params()['classifier'])\n",
    "\n",
    "    cv_score = clf.cv_results_['mean_test_score']\n",
    "\n",
    "    print(\"Grid search cv scores:\", cv_score)\n",
    "\n",
    "    predicted = best_model.predict(X_val)\n",
    "    print(\"Acuracy on validation:\", np.mean(predicted == y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-28T14:48:04.205972Z",
     "start_time": "2018-07-28T14:40:55.931Z"
    }
   },
   "outputs": [],
   "source": [
    "# todo create an ensemble classifier and a stacking classifier with pipelines and stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Consumer_complaint_narrative'], df['Product'], random_state = 0)\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using spacy textclassifier - neural nets!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example code from docs\n",
    "\n",
    "https://spacy.io/usage/training#section-textcat\n",
    "\n",
    "https://spacy.io/api/textcategorizer#call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-28T14:48:04.207624Z",
     "start_time": "2018-07-28T14:40:55.949Z"
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf8\n",
    "\"\"\"Train a convolutional neural network text classifier on the\n",
    "IMDB dataset, using the TextCategorizer component. The dataset will be loaded\n",
    "automatically via Thinc's built-in dataset loader. The model is added to\n",
    "spacy.pipeline, and predictions are available via `doc.cats`. For more details,\n",
    "see the documentation:\n",
    "* Training: https://spacy.io/usage/training\n",
    "\n",
    "Compatible with: spaCy v2.0.0+\n",
    "\"\"\"\n",
    "from __future__ import unicode_literals, print_function\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import thinc.extra.datasets\n",
    "\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "\n",
    "@plac.annotations(\n",
    "    model=(\"Model name. Defaults to blank 'en' model.\", \"option\", \"m\", str),\n",
    "    output_dir=(\"Optional output directory\", \"option\", \"o\", Path),\n",
    "    n_texts=(\"Number of texts to train from\", \"option\", \"t\", int),\n",
    "    n_iter=(\"Number of training iterations\", \"option\", \"n\", int))\n",
    "def main(model=None, output_dir=None, n_iter=20, n_texts=2000):\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank('en')  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "\n",
    "    # add the text classifier to the pipeline if it doesn't exist\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if 'textcat' not in nlp.pipe_names:\n",
    "        textcat = nlp.create_pipe('textcat')\n",
    "        nlp.add_pipe(textcat, last=True)\n",
    "    # otherwise, get it, so we can add labels to it\n",
    "    else:\n",
    "        textcat = nlp.get_pipe('textcat')\n",
    "\n",
    "    # add label to text classifier\n",
    "    textcat.add_label('POSITIVE')\n",
    "\n",
    "    # load the IMDB dataset\n",
    "    print(\"Loading IMDB data...\")\n",
    "    (train_texts, train_cats), (dev_texts, dev_cats) = load_data(limit=n_texts)\n",
    "    print(\"Using {} examples ({} training, {} evaluation)\"\n",
    "          .format(n_texts, len(train_texts), len(dev_texts)))\n",
    "    train_data = list(zip(train_texts,\n",
    "                          [{'cats': cats} for cats in train_cats]))\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'textcat']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train textcat\n",
    "        optimizer = nlp.begin_training()\n",
    "        print(\"Training the model...\")\n",
    "        print('{:^5}\\t{:^5}\\t{:^5}\\t{:^5}'.format('LOSS', 'P', 'R', 'F'))\n",
    "        for i in range(n_iter):\n",
    "            losses = {}\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            batches = minibatch(train_data, size=compounding(4., 32., 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(texts, annotations, sgd=optimizer, drop=0.2,\n",
    "                           losses=losses)\n",
    "            with textcat.model.use_params(optimizer.averages):\n",
    "                # evaluate on the dev data split off in load_data()\n",
    "                scores = evaluate(nlp.tokenizer, textcat, dev_texts, dev_cats)\n",
    "            print('{0:.3f}\\t{1:.3f}\\t{2:.3f}\\t{3:.3f}'  # print a simple table\n",
    "                  .format(losses['textcat'], scores['textcat_p'],\n",
    "                          scores['textcat_r'], scores['textcat_f']))\n",
    "\n",
    "    # test the trained model\n",
    "    test_text = \"This movie sucked\"\n",
    "    doc = nlp(test_text)\n",
    "    print(test_text, doc.cats)\n",
    "\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "        # test the saved model\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        doc2 = nlp2(test_text)\n",
    "        print(test_text, doc2.cats)\n",
    "\n",
    "\n",
    "def load_data(limit=0, split=0.8):\n",
    "    \"\"\"Load data from the IMDB dataset.\"\"\"\n",
    "    # Partition off part of the train data for evaluation\n",
    "    train_data, _ = thinc.extra.datasets.imdb()\n",
    "    random.shuffle(train_data)\n",
    "    train_data = train_data[-limit:]\n",
    "    texts, labels = zip(*train_data)\n",
    "    cats = [{'POSITIVE': bool(y)} for y in labels]\n",
    "    split = int(len(train_data) * split)\n",
    "    return (texts[:split], cats[:split]), (texts[split:], cats[split:])\n",
    "\n",
    "\n",
    "def evaluate(tokenizer, textcat, texts, cats):\n",
    "    docs = (tokenizer(text) for text in texts)\n",
    "    tp = 1e-8  # True positives\n",
    "    fp = 1e-8  # False positives\n",
    "    fn = 1e-8  # False negatives\n",
    "    tn = 1e-8  # True negatives\n",
    "    for i, doc in enumerate(textcat.pipe(docs)):\n",
    "        gold = cats[i]\n",
    "        for label, score in doc.cats.items():\n",
    "            if label not in gold:\n",
    "                continue\n",
    "            if score >= 0.5 and gold[label] >= 0.5:\n",
    "                tp += 1.\n",
    "            elif score >= 0.5 and gold[label] < 0.5:\n",
    "                fp += 1.\n",
    "            elif score < 0.5 and gold[label] < 0.5:\n",
    "                tn += 1\n",
    "            elif score < 0.5 and gold[label] >= 0.5:\n",
    "                fn += 1\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return {'textcat_p': precision, 'textcat_r': recall, 'textcat_f': f_score}\n",
    "\n",
    "\n",
    "#main(model=None, output_dir='/tmp/dockerwd/spacy_models/model2', n_iter=10, n_texts=2500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code adapted to work with external dataframe - spam example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-28T14:48:04.209980Z",
     "start_time": "2018-07-28T14:40:55.956Z"
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf8\n",
    "\"\"\"Train a convolutional neural network text classifier using the TextCategorizer component. \n",
    "The model is added to spacy.pipeline, and predictions are available via `doc.cats`. For more details,\n",
    "see the documentation:\n",
    "* Training: https://spacy.io/usage/training\n",
    "\n",
    "Compatible with: spaCy v2.0.0+\n",
    "\"\"\"\n",
    "from __future__ import unicode_literals, print_function\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import thinc.extra.datasets\n",
    "\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "\n",
    "@plac.annotations(\n",
    "    model=(\"Model name. Defaults to blank 'en' model.\", \"option\", \"m\", str),\n",
    "    output_dir=(\"Optional output directory\", \"option\", \"o\", Path),\n",
    "    n_texts=(\"Number of texts to train from\", \"option\", \"t\", int),\n",
    "    n_iter=(\"Number of training iterations\", \"option\", \"n\", int))\n",
    "def main(_train_df, model=None, output_dir=None, n_iter=20, n_texts=0, text_column_name ='message',label_column_name='label'):\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank('en')  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "\n",
    "    # add the text classifier to the pipeline if it doesn't exist\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if 'textcat' not in nlp.pipe_names:\n",
    "        textcat = nlp.create_pipe('textcat')\n",
    "        nlp.add_pipe(textcat, last=True)\n",
    "    # otherwise, get it, so we can add labels to it\n",
    "    else:\n",
    "        textcat = nlp.get_pipe('textcat')\n",
    "\n",
    "    # add label to text classifier\n",
    "    textcat.add_label('SPAM')\n",
    "\n",
    "    # load the IMDB dataset\n",
    "    print(\"Loading data...\")\n",
    "    (train_texts, train_cats), (dev_texts, dev_cats) = load_data(_train_df, text_column_name, \n",
    "                                                                 label_column_name, limit=n_texts)\n",
    "    \n",
    "    print(\"Using {} examples ({} training, {} evaluation)\"\n",
    "          .format(n_texts, len(train_texts), len(dev_texts)))\n",
    "    train_data = list(zip(train_texts,\n",
    "                          [{'cats': cats} for cats in train_cats]))\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'textcat']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train textcat\n",
    "        optimizer = nlp.begin_training()\n",
    "        print(\"Training the model...\")\n",
    "        print('{:^5}\\t{:^5}\\t{:^5}\\t{:^5}\\t{:^5}'.format('LOSS', 'A', 'P', 'R', 'F'))\n",
    "        for i in range(n_iter):\n",
    "            losses = {}\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            batches = minibatch(train_data, size=compounding(4., 32., 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(texts, annotations, sgd=optimizer, drop=0.2,\n",
    "                           losses=losses)\n",
    "            with textcat.model.use_params(optimizer.averages):\n",
    "                # evaluate on the dev data split off in load_data()\n",
    "                scores = evaluate(nlp.tokenizer, textcat, dev_texts, dev_cats)\n",
    "            print('{0:.3f}\\t{1:.3f}\\t{2:.3f}\\t{3:.3f}\\t{4:.3f}'  # print a simple table\n",
    "                  .format(losses['textcat'], scores['textcat_a'], scores['textcat_p'],\n",
    "                          scores['textcat_r'], scores['textcat_f']))\n",
    "\n",
    "    # test the trained model\n",
    "    test_text = \"BIG DICKZZ!! GIRLZZ!! VISIT www.win-454250.co.uk\"\n",
    "    doc = nlp(test_text)\n",
    "    print(test_text, doc.cats)\n",
    "\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "        # test the saved model\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        doc2 = nlp2(test_text)\n",
    "        print(test_text, doc2.cats)\n",
    "        \n",
    "    return nlp\n",
    "\n",
    "\n",
    "def load_data(_train_df, text_column_name, label_column_name, limit=0, split=0.8):\n",
    "    \"\"\"Load dataset.\"\"\"\n",
    "    # Partition off part of the train data for evaluation\n",
    "    #train_data, _ = thinc.extra.datasets.imdb()\n",
    "    train_data = [(row[text_column_name], row[label_column_name]) for index, row in _train_df.iterrows()]\n",
    "    random.shuffle(train_data)\n",
    "    #print(train_data[0:10])\n",
    "    train_data = train_data[-limit:]\n",
    "    texts, labels = zip(*train_data)\n",
    "    cats = [{'SPAM': bool(y =='spam')} for y in labels]\n",
    "    split = int(len(train_data) * split)\n",
    "    return (texts[:split], cats[:split]), (texts[split:], cats[split:])\n",
    "\n",
    "\n",
    "def evaluate(tokenizer, textcat, texts, cats):\n",
    "    docs = (tokenizer(text) for text in texts)\n",
    "    tp = 1e-8  # True positives\n",
    "    fp = 1e-8  # False positives\n",
    "    fn = 1e-8  # False negatives\n",
    "    tn = 1e-8  # True negatives\n",
    "    for i, doc in enumerate(textcat.pipe(docs)):\n",
    "        gold = cats[i]\n",
    "        for label, score in doc.cats.items():\n",
    "            if label not in gold:\n",
    "                continue\n",
    "            if score >= 0.5 and gold[label] >= 0.5:\n",
    "                tp += 1.\n",
    "            elif score >= 0.5 and gold[label] < 0.5:\n",
    "                fp += 1.\n",
    "            elif score < 0.5 and gold[label] < 0.5:\n",
    "                tn += 1\n",
    "            elif score < 0.5 and gold[label] >= 0.5:\n",
    "                fn += 1\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f_score = 2 * (precision * recall) / (precision + recall)\n",
    "    accuracy = (tp + tn) /(tp + tn + fp + fn)\n",
    "    return {'textcat_a' : accuracy, 'textcat_p': precision, 'textcat_r': recall, 'textcat_f': f_score}\n",
    "\n",
    "\n",
    "def spacy_predict(_nlp, _test_data, label, predict_proba=True):\n",
    "    predictions = []\n",
    "    if predict_proba:\n",
    "        for doc in _test_data.values:\n",
    "            predictions.append(_nlp(doc).cats[label])\n",
    "    else:\n",
    "        for doc in _test_data.values:\n",
    "            predictions.append(int((_nlp(doc).cats[label]) > 0.5))\n",
    "    return pd.Series(predictions)\n",
    "\n",
    "\n",
    "df = pd.read_csv('data_for_notes/spam.csv', encoding='latin1')\n",
    "df.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1,inplace=True)\n",
    "df.rename(columns={\"v1\":\"label\", \"v2\":\"message\"},inplace=True)\n",
    "\n",
    "\n",
    "#nlp = main(df, model=None, output_dir='/tmp/dockerwd/spacy_models/model3', n_iter=5)\n",
    "#nlp = main(df, model='en_core_web_lg', output_dir='/tmp/dockerwd/spacy_models/model4', n_iter=5)\n",
    "#nlp = main(df, model='/tmp/dockerwd/spacy_models/model4', output_dir='/tmp/dockerwd/spacy_models/model5', n_iter=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-28T14:48:04.211390Z",
     "start_time": "2018-07-28T14:40:55.960Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_for_notes/spam.csv', encoding='latin1')\n",
    "df.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1,inplace=True)\n",
    "df.rename(columns={\"v1\":\"label\", \"v2\":\"message\"},inplace=True)\n",
    "df.head()\n",
    "(train_texts, train_cats), (dev_texts, dev_cats) = load_data(df)\n",
    "len(train_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adapted for other problem as example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-28T14:48:04.212690Z",
     "start_time": "2018-07-28T14:40:55.965Z"
    }
   },
   "outputs": [],
   "source": [
    "new_df = pd.read_csv('data_for_notes/imdb_sentiment.csv')\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-28T14:48:04.214515Z",
     "start_time": "2018-07-28T14:40:55.971Z"
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf8\n",
    "\"\"\"Train a convolutional neural network text classifier using the TextCategorizer component. \n",
    "The model is added to spacy.pipeline, and predictions are available via `doc.cats`. For more details,\n",
    "see the documentation:\n",
    "* Training: https://spacy.io/usage/training\n",
    "\n",
    "Compatible with: spaCy v2.0.0+\n",
    "\"\"\"\n",
    "from __future__ import unicode_literals, print_function\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import thinc.extra.datasets\n",
    "\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "\n",
    "@plac.annotations(\n",
    "    model=(\"Model name. Defaults to blank 'en' model.\", \"option\", \"m\", str),\n",
    "    output_dir=(\"Optional output directory\", \"option\", \"o\", Path),\n",
    "    n_texts=(\"Number of texts to train from\", \"option\", \"t\", int),\n",
    "    n_iter=(\"Number of training iterations\", \"option\", \"n\", int))\n",
    "def main(_train_df, model=None, output_dir=None, n_iter=20, n_texts=0, text_column_name ='message',label_column_name='label'):\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank('en')  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "\n",
    "    # add the text classifier to the pipeline if it doesn't exist\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if 'textcat' not in nlp.pipe_names:\n",
    "        textcat = nlp.create_pipe('textcat')\n",
    "        nlp.add_pipe(textcat, last=True)\n",
    "    # otherwise, get it, so we can add labels to it\n",
    "    else:\n",
    "        textcat = nlp.get_pipe('textcat')\n",
    "\n",
    "    # add label to text classifier\n",
    "    textcat.add_label('POSITIVE')\n",
    "\n",
    "    # load the IMDB dataset\n",
    "    print(\"Loading data...\")\n",
    "    (train_texts, train_cats), (dev_texts, dev_cats) = load_data(_train_df, text_column_name, \n",
    "                                                                 label_column_name, limit=n_texts)\n",
    "    \n",
    "    print(\"Using {} examples ({} training, {} evaluation)\"\n",
    "          .format(n_texts, len(train_texts), len(dev_texts)))\n",
    "    train_data = list(zip(train_texts,\n",
    "                          [{'cats': cats} for cats in train_cats]))\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'textcat']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train textcat\n",
    "        optimizer = nlp.begin_training()\n",
    "        print(\"Training the model...\")\n",
    "        print('{:^5}\\t{:^5}\\t{:^5}\\t{:^5}\\t{:^5}'.format('LOSS', 'A', 'P', 'R', 'F'))\n",
    "        for i in range(n_iter):\n",
    "            losses = {}\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            batches = minibatch(train_data, size=compounding(4., 32., 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(texts, annotations, sgd=optimizer, drop=0.2,\n",
    "                           losses=losses)\n",
    "            with textcat.model.use_params(optimizer.averages):\n",
    "                # evaluate on the dev data split off in load_data()\n",
    "                scores = evaluate(nlp.tokenizer, textcat, dev_texts, dev_cats)\n",
    "            print('{0:.3f}\\t{1:.3f}\\t{2:.3f}\\t{3:.3f}\\t{4:.3f}'  # print a simple table\n",
    "                  .format(losses['textcat'], scores['textcat_a'], scores['textcat_p'],\n",
    "                          scores['textcat_r'], scores['textcat_f']))\n",
    "\n",
    "    # test the trained model\n",
    "    test_text = \"This movie sucked! Crapiest movie ever! Waste of money!\"\n",
    "    doc = nlp(test_text)\n",
    "    print(test_text, doc.cats)\n",
    "\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "        # test the saved model\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        doc2 = nlp2(test_text)\n",
    "        print(test_text, doc2.cats)\n",
    "        \n",
    "    return nlp\n",
    "\n",
    "\n",
    "def load_data(_train_df, text_column_name, label_column_name, limit=0, split=0.8):\n",
    "    \"\"\"Load data from the IMDB dataset.\"\"\"\n",
    "    # Partition off part of the train data for evaluation\n",
    "    #train_data, _ = thinc.extra.datasets.imdb()\n",
    "    train_data = [(row[text_column_name], row[label_column_name]) for index, row in _train_df.iterrows()]\n",
    "    random.shuffle(train_data)\n",
    "    #print(train_data[0:10])\n",
    "    train_data = train_data[-limit:]\n",
    "    texts, labels = zip(*train_data)\n",
    "    cats = [{'POSITIVE': bool(y =='Positive')} for y in labels]\n",
    "    split = int(len(train_data) * split)\n",
    "    return (texts[:split], cats[:split]), (texts[split:], cats[split:])\n",
    "\n",
    "\n",
    "def evaluate(tokenizer, textcat, texts, cats):\n",
    "    docs = (tokenizer(text) for text in texts)\n",
    "    tp = 1e-8  # True positives\n",
    "    fp = 1e-8  # False positives\n",
    "    fn = 1e-8  # False negatives\n",
    "    tn = 1e-8  # True negatives\n",
    "    for i, doc in enumerate(textcat.pipe(docs)):\n",
    "        gold = cats[i]\n",
    "        for label, score in doc.cats.items():\n",
    "            if label not in gold:\n",
    "                continue\n",
    "            if score >= 0.5 and gold[label] >= 0.5:\n",
    "                tp += 1.\n",
    "            elif score >= 0.5 and gold[label] < 0.5:\n",
    "                fp += 1.\n",
    "            elif score < 0.5 and gold[label] < 0.5:\n",
    "                tn += 1\n",
    "            elif score < 0.5 and gold[label] >= 0.5:\n",
    "                fn += 1\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f_score = 2 * (precision * recall) / (precision + recall)\n",
    "    accuracy = (tp + tn) /(tp + tn + fp + fn)\n",
    "    return {'textcat_a' : accuracy, 'textcat_p': precision, 'textcat_r': recall, 'textcat_f': f_score}\n",
    "\n",
    "def spacy_predict(_nlp, _test_data, label, predict_proba=True):\n",
    "    predictions = []\n",
    "    if predict_proba:\n",
    "        for doc in _test_data.values:\n",
    "            predictions.append(_nlp(doc).cats[label])\n",
    "    else:\n",
    "        for doc in _test_data.values:\n",
    "            predictions.append(int((_nlp(doc).cats[label]) > 0.5))\n",
    "    return pd.Series(predictions)\n",
    "\n",
    "\n",
    "\n",
    "#new_df = pd.read_csv('data_for_notes/imdb_sentiment.csv')\n",
    "\n",
    "#nlp = main(new_df, model=None, output_dir='/tmp/dockerwd/spacy_models/model6', n_iter=5, \n",
    "#      text_column_name ='text',label_column_name='sentiment')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-28T14:48:04.216221Z",
     "start_time": "2018-07-28T14:40:55.975Z"
    }
   },
   "outputs": [],
   "source": [
    "#print(spacy_predict(nlp, new_df['text'], 'POSITIVE'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adapting for a multiclassification problem\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf8\n",
    "\"\"\"Train a convolutional neural network text classifier using the TextCategorizer component. \n",
    "The model is added to spacy.pipeline, and predictions are available via `doc.cats`. For more details,\n",
    "see the documentation:\n",
    "* Training: https://spacy.io/usage/training\n",
    "\n",
    "Compatible with: spaCy v2.0.0+\n",
    "\"\"\"\n",
    "from __future__ import unicode_literals, print_function\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import thinc.extra.datasets\n",
    "\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "\n",
    "@plac.annotations(\n",
    "    model=(\"Model name. Defaults to blank 'en' model.\", \"option\", \"m\", str),\n",
    "    output_dir=(\"Optional output directory\", \"option\", \"o\", Path),\n",
    "    n_texts=(\"Number of texts to train from\", \"option\", \"t\", int),\n",
    "    n_iter=(\"Number of training iterations\", \"option\", \"n\", int))\n",
    "def main(_train_df, model=None, output_dir=None, n_iter=20, n_texts=0, text_column_name ='message',label_column_name='label'):\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank('en')  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "\n",
    "    # add the text classifier to the pipeline if it doesn't exist\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if 'textcat' not in nlp.pipe_names:\n",
    "        textcat = nlp.create_pipe('textcat')\n",
    "        nlp.add_pipe(textcat, last=True)\n",
    "    # otherwise, get it, so we can add labels to it\n",
    "    else:\n",
    "        textcat = nlp.get_pipe('textcat')\n",
    "\n",
    "    # add label to text classifier\n",
    "    textcat.add_label('POSITIVE')\n",
    "\n",
    "    # load the IMDB dataset\n",
    "    print(\"Loading data...\")\n",
    "    (train_texts, train_cats), (dev_texts, dev_cats) = load_data(_train_df, text_column_name, \n",
    "                                                                 label_column_name, limit=n_texts)\n",
    "    \n",
    "    print(\"Using {} examples ({} training, {} evaluation)\"\n",
    "          .format(n_texts, len(train_texts), len(dev_texts)))\n",
    "    train_data = list(zip(train_texts,\n",
    "                          [{'cats': cats} for cats in train_cats]))\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'textcat']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train textcat\n",
    "        optimizer = nlp.begin_training()\n",
    "        print(\"Training the model...\")\n",
    "        print('{:^5}\\t{:^5}\\t{:^5}\\t{:^5}\\t{:^5}'.format('LOSS', 'A', 'P', 'R', 'F'))\n",
    "        for i in range(n_iter):\n",
    "            losses = {}\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            batches = minibatch(train_data, size=compounding(4., 32., 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(texts, annotations, sgd=optimizer, drop=0.2,\n",
    "                           losses=losses)\n",
    "            with textcat.model.use_params(optimizer.averages):\n",
    "                # evaluate on the dev data split off in load_data()\n",
    "                scores = evaluate(nlp.tokenizer, textcat, dev_texts, dev_cats)\n",
    "            print('{0:.3f}\\t{1:.3f}\\t{2:.3f}\\t{3:.3f}\\t{4:.3f}'  # print a simple table\n",
    "                  .format(losses['textcat'], scores['textcat_a'], scores['textcat_p'],\n",
    "                          scores['textcat_r'], scores['textcat_f']))\n",
    "\n",
    "    # test the trained model\n",
    "    test_text = \"This movie sucked! Crapiest movie ever! Waste of money!\"\n",
    "    doc = nlp(test_text)\n",
    "    print(test_text, doc.cats)\n",
    "\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "        # test the saved model\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        doc2 = nlp2(test_text)\n",
    "        print(test_text, doc2.cats)\n",
    "        \n",
    "    return nlp\n",
    "\n",
    "\n",
    "def load_data(_train_df, text_column_name, label_column_name, limit=0, split=0.8):\n",
    "    \"\"\"Load data from the IMDB dataset.\"\"\"\n",
    "    # Partition off part of the train data for evaluation\n",
    "    #train_data, _ = thinc.extra.datasets.imdb()\n",
    "    train_data = [(row[text_column_name], row[label_column_name]) for index, row in _train_df.iterrows()]\n",
    "    random.shuffle(train_data)\n",
    "    #print(train_data[0:10])\n",
    "    train_data = train_data[-limit:]\n",
    "    texts, labels = zip(*train_data)\n",
    "    cats = [{'POSITIVE': bool(y =='Positive')} for y in labels]\n",
    "    split = int(len(train_data) * split)\n",
    "    return (texts[:split], cats[:split]), (texts[split:], cats[split:])\n",
    "\n",
    "\n",
    "def evaluate(tokenizer, textcat, texts, cats):\n",
    "    docs = (tokenizer(text) for text in texts)\n",
    "    tp = 1e-8  # True positives\n",
    "    fp = 1e-8  # False positives\n",
    "    fn = 1e-8  # False negatives\n",
    "    tn = 1e-8  # True negatives\n",
    "    for i, doc in enumerate(textcat.pipe(docs)):\n",
    "        gold = cats[i]\n",
    "        for label, score in doc.cats.items():\n",
    "            if label not in gold:\n",
    "                continue\n",
    "            if score >= 0.5 and gold[label] >= 0.5:\n",
    "                tp += 1.\n",
    "            elif score >= 0.5 and gold[label] < 0.5:\n",
    "                fp += 1.\n",
    "            elif score < 0.5 and gold[label] < 0.5:\n",
    "                tn += 1\n",
    "            elif score < 0.5 and gold[label] >= 0.5:\n",
    "                fn += 1\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f_score = 2 * (precision * recall) / (precision + recall)\n",
    "    accuracy = (tp + tn) /(tp + tn + fp + fn)\n",
    "    return {'textcat_a' : accuracy, 'textcat_p': precision, 'textcat_r': recall, 'textcat_f': f_score}\n",
    "\n",
    "def spacy_predict(_nlp, _test_data, label, predict_proba=True):\n",
    "    predictions = []\n",
    "    if predict_proba:\n",
    "        for doc in _test_data.values:\n",
    "            predictions.append(_nlp(doc).cats[label])\n",
    "    else:\n",
    "        for doc in _test_data.values:\n",
    "            predictions.append(int((_nlp(doc).cats[label]) > 0.5))\n",
    "    return pd.Series(predictions)\n",
    "\n",
    "\n",
    "\n",
    "#new_df = pd.read_csv('data_for_notes/imdb_sentiment.csv')\n",
    "\n",
    "#nlp = main(new_df, model=None, output_dir='/tmp/dockerwd/spacy_models/model6', n_iter=5, \n",
    "#      text_column_name ='text',label_column_name='sentiment')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
