{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package rslp to\n",
      "[nltk_data]     /Users/manuelcostareis/nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('rslp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import string\n",
    "from numpy import inf\n",
    "\n",
    "# Baseline \n",
    "from collections import Counter, OrderedDict\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem import RSLPStemmer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from xgboost import XGBClassifier\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('portuguese')\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "\n",
    "# SKLearn related imports\n",
    "import sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "import spacy\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['add_to_playlist', 'search_screening_event', 'book_restaurant',\n",
       "       'rate_book', 'get_weather', 'play_music', 'search_creative_work',\n",
       "       'no_intent'], dtype=object)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Request</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adicionar música tomohisa yamashita à minha li...</td>\n",
       "      <td>add_to_playlist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Eu quero adicionando Aprite le finestre à minh...</td>\n",
       "      <td>add_to_playlist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Que curta-metragens estão tocando às 11 da man...</td>\n",
       "      <td>search_screening_event</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>precisa de uma sala em um que serve foie gras ...</td>\n",
       "      <td>book_restaurant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Atribuir 4 estrelas de 6 para a crônica, Deus ...</td>\n",
       "      <td>rate_book</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Request                   Label\n",
       "0  Adicionar música tomohisa yamashita à minha li...         add_to_playlist\n",
       "1  Eu quero adicionando Aprite le finestre à minh...         add_to_playlist\n",
       "2  Que curta-metragens estão tocando às 11 da man...  search_screening_event\n",
       "3  precisa de uma sala em um que serve foie gras ...         book_restaurant\n",
       "4  Atribuir 4 estrelas de 6 para a crônica, Deus ...               rate_book"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Request</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>26527</td>\n",
       "      <td>26799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>25752</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Encontre o horário do filme.</td>\n",
       "      <td>no_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>5</td>\n",
       "      <td>12645</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Request      Label\n",
       "count                          26527      26799\n",
       "unique                         25752          8\n",
       "top     Encontre o horário do filme.  no_intent\n",
       "freq                               5      12645"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Toca a melhor música de Phoebe Snow'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Request.iloc[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('pt_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_aditional_POS_features(_df, text_column_name,_nlp, return_only_new_feats = False):\n",
    "    n_adj, n_verbs, len_message= [], [], []\n",
    "    _df_copy = _df.copy()\n",
    "    n_adv = []\n",
    "    n_noun = [] \n",
    "    \n",
    "    for doc in _nlp.pipe(_df_copy[text_column_name]):\n",
    "        n_adj.append(len([token for token in doc if token.pos_ == 'ADJ']))\n",
    "        if len([token for token in doc if token.pos_ == 'ADJ']) is np.nan:\n",
    "            print(doc)\n",
    "            \n",
    "        n_verbs.append(len([token for token in doc if token.pos_ == 'VERB']))\n",
    "        \n",
    "        # Acrescentados adverbs ADV , nouns NOUN\n",
    "        n_adv.append(len([token for token in doc if token.pos_ == 'ADV']))\n",
    "        n_noun.append(len([token for token in doc if token.pos_ == 'NOUN']))\n",
    "        \n",
    "        len_message.append(len(doc))\n",
    "    _df_copy['n_adj'] = pd.Series(n_adj)\n",
    "    _df_copy['n_verbs'] = pd.Series(n_verbs)\n",
    "    _df_copy['n_adv'] = pd.Series(n_adv)\n",
    "    _df_copy['n_noun'] = pd.Series(n_noun)\n",
    "    _df_copy['len_text'] = pd.Series(len_message)\n",
    "    \n",
    "    if return_only_new_feats:\n",
    "        return _df_copy.drop(text_column_name, axis=1)\n",
    "    else:\n",
    "        return _df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "df = create_aditional_POS_features(df, 'Request', nlp)\n",
    "df = df.dropna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[['Request', 'n_adj', 'n_verbs', 'n_adv', 'n_noun', 'len_text']], df.Label, test_size=0.33, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordPunctTokenizer()\n",
    "stemmer = RSLPStemmer()\n",
    "regex_list = [(\"<[^>]*>\", \"\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and Stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer to implement sentence cleaning\n",
    "class PortugueseCleaner(TransformerMixin):\n",
    "    def __init__(self, tokenizer, stemmer, regex_list,\n",
    "                 lower=True, remove_punct=True):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stemmer = stemmer\n",
    "        self.regex_list = regex_list\n",
    "        self.lower = lower\n",
    "        self.remove_punct = remove_punct\n",
    "        \n",
    "    def transform(self, X, *_):\n",
    "        X = list(map(self._clean_sentence, X.Request))\n",
    "        return X\n",
    "    \n",
    "    def _clean_sentence(self, sentence):\n",
    "        # lowercase\n",
    "        if self.lower:\n",
    "            sentence = sentence.lower()\n",
    "\n",
    "        # Split sentence into list of words\n",
    "        words = self.tokenizer.tokenize(sentence)\n",
    "            \n",
    "        # Remove punctuation\n",
    "        if self.remove_punct:\n",
    "            words = list(filter(lambda x: x not in string.punctuation, words))\n",
    "\n",
    "        # Stem words\n",
    "        if self.stemmer:\n",
    "            words = map(self.stemmer.stem, words)\n",
    "\n",
    "        # Join list elements into string\n",
    "        sentence = \" \".join(words)\n",
    "        \n",
    "        return sentence\n",
    "    \n",
    "    def fit(self, *_):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class TextSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on text columns in the data\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[self.key]\n",
    "    \n",
    "class NumberSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on numeric columns in the data\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[[self.key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeliner():\n",
    "    k = 20000\n",
    "    tokenizer = WordPunctTokenizer()\n",
    "    stemmer = RSLPStemmer()\n",
    "    regex_list = [(\"<[^>]*>\", \"\")]\n",
    " \n",
    "\n",
    "    text = Pipeline([('stemm', PortugueseCleaner(tokenizer, stemmer, regex_list, lower=False)),\n",
    "                     ('vect', TfidfVectorizer(ngram_range=(1,2))),\n",
    "                     ('selectKBest', SelectKBest(chi2, k=k)),\n",
    "                     ])\n",
    "\n",
    "    adj =  Pipeline([\n",
    "                    ('selector', NumberSelector(key='n_adj')),\n",
    "                    ('standard', StandardScaler())\n",
    "                ])\n",
    "\n",
    "    verbs =  Pipeline([\n",
    "                    ('selector', NumberSelector(key='n_verbs')),\n",
    "                    ('standard', StandardScaler())\n",
    "                ])\n",
    "    \n",
    "    adv =  Pipeline([\n",
    "                    ('selector', NumberSelector(key='n_adv')),\n",
    "                    ('standard', StandardScaler())\n",
    "                ])\n",
    "\n",
    "    noun =  Pipeline([\n",
    "                    ('selector', NumberSelector(key='n_noun')),\n",
    "                    ('standard', StandardScaler())\n",
    "                ])\n",
    "    \n",
    "    \n",
    "    len_text =  Pipeline([\n",
    "                    ('selector', NumberSelector(key='len_text')),\n",
    "                    ('standard', StandardScaler())\n",
    "                ])\n",
    "\n",
    "    feats = FeatureUnion([('Request', text), \n",
    "                          ('n_adj', adj),\n",
    "                          ('n_verbs', verbs),\n",
    "                          ('n_adv', adv),\n",
    "                          ('n_noun', noun),\n",
    "                          ('len_text', len_text)\n",
    "                           ])\n",
    "\n",
    "    return Pipeline([('features', feats),\n",
    "                     ('classifier', XGBClassifier(n_jobs=6, n_estimators=2000))])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeliner()\n",
    "# Train the classifier\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Request     previsão para Hardinsburg 20 segundos a partir...\n",
       "n_adj                                                       0\n",
       "n_verbs                                                     0\n",
       "n_adv                                                       1\n",
       "n_noun                                                      2\n",
       "len_text                                                   13\n",
       "Name: 3224, dtype: object"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = pipe.predict(X_test)\n",
    "np.mean(predicted == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeliner()\n",
    "# Train the classifier\n",
    "pipe.fit(df.Request, df.Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = pipe.predict(test.Request)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Label'] = predicted\n",
    "test = test.set_index('Request')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('../data/submission3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
